{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"9606.hn_HS_CX.edge\", sep=\"\\t\", header=None)\n",
    "data.drop([3, 4, 5], axis=1, inplace=True)\n",
    "data.columns = [\"node1\", \"node2\", \"weight\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = np.array(\n",
    "    list(set(data[\"node1\"].unique()).union(set(data[\"node2\"].unique())))\n",
    ")\n",
    "print(\"Total number of nodes: {}\".format(len(all_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First two nodes :\n",
    "\n",
    "* ENSG00000284589\n",
    "* ENSG00000276821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_symmetric_adjacency_matrix(df):\n",
    "    \"\"\"\n",
    "    Create a symmetric adjacency matrix from a dataframe containing edges and weights.\n",
    "    \"\"\"\n",
    "    df_symmetric = pd.concat(\n",
    "        [df, df.rename(columns={\"node1\": \"node2\", \"node2\": \"node1\"})]\n",
    "    )\n",
    "    df_symmetric = df_symmetric.groupby([\"node1\", \"node2\"]).weight.mean().reset_index()\n",
    "\n",
    "    adjacency_matrix = df_symmetric.pivot(\n",
    "        index=\"node1\", columns=\"node2\", values=\"weight\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    matrix_np = adjacency_matrix.to_numpy()\n",
    "    symmetrized_matrix_np = matrix_np + matrix_np.T - np.diag(matrix_np.diagonal())\n",
    "\n",
    "    return symmetrized_matrix_np\n",
    "\n",
    "\n",
    "def remove_self_loops_and_small_components(adj_matrix, threshold_components=4):\n",
    "    \"\"\"\n",
    "    Remove self-loops and small disconnected components from the graph represented by the adjacency matrix.\n",
    "    Also, keep track of the removed nodes.\n",
    "    \"\"\"\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    removed_nodes = []\n",
    "\n",
    "    for edge in nx.selfloop_edges(G):\n",
    "        G.remove_edge(*edge)\n",
    "\n",
    "    components = list(nx.connected_components(G))\n",
    "    for component in components:\n",
    "        if len(component) < threshold_components:\n",
    "            removed_nodes.extend(component)\n",
    "            for node in component:\n",
    "                G.remove_node(node)\n",
    "\n",
    "    cleaned_adj_matrix = nx.to_numpy_array(G)\n",
    "\n",
    "    return cleaned_adj_matrix, set(removed_nodes)\n",
    "\n",
    "\n",
    "def normalize_to_stochastic_matrix(adj_matrix):\n",
    "    \"\"\"\n",
    "    Normalize an adjacency matrix so that each row sums to 1, creating a stochastic matrix.\n",
    "    \"\"\"\n",
    "    matrix_np = np.array(adj_matrix)\n",
    "\n",
    "    row_sums = matrix_np.sum(axis=1, keepdims=True)\n",
    "\n",
    "    row_sums[row_sums == 0] = 1\n",
    "\n",
    "    stochastic_matrix = matrix_np / row_sums\n",
    "\n",
    "    return stochastic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat = create_symmetric_adjacency_matrix(data)\n",
    "cleaned_adj_mat, removed_nodes = remove_self_loops_and_small_components(adj_mat)\n",
    "\n",
    "removed_nodes = list(removed_nodes)\n",
    "all_nodes_cleaned = np.delete(all_nodes, removed_nodes)\n",
    "avg_degree = np.count_nonzero(cleaned_adj_mat, axis=1).mean()\n",
    "\n",
    "print(f\"Number of removed nodes: {len(removed_nodes)}\")\n",
    "print(f\"Number of nodes after cleaning: {len(all_nodes_cleaned)}\")\n",
    "print(f\"Average degree: {avg_degree:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned graph and list of nodes\n",
    "G = nx.from_numpy_array(cleaned_adj_mat)\n",
    "nx.write_weighted_edgelist(G, \"./cleaned_graph.edge\")\n",
    "joblib.dump(all_nodes_cleaned, \"./cleaned_nodes_arr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_matrix = normalize_to_stochastic_matrix(cleaned_adj_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Without Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_without_restart_to_stationary_distribution(\n",
    "    stochastic_matrix, start_node, threshold=1e-4, max_iterations=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a random walk on a graph represented by a stochastic matrix, starting from a given node,\n",
    "    until the stationary distribution is reached or the maximum number of iterations is exceeded.\n",
    "\n",
    "    :param stochastic_matrix: The stochastic matrix representing the graph.\n",
    "    :param start_node: The index of the starting node.\n",
    "    :param threshold: The threshold for the difference between successive distribution vectors.\n",
    "    :param max_iterations: The maximum number of iterations to perform.\n",
    "    :return: The stationary distribution vector of the random walk.\n",
    "    \"\"\"\n",
    "    num_nodes = stochastic_matrix.shape[0]\n",
    "\n",
    "    q_k = np.zeros(num_nodes)\n",
    "    q_k[start_node] = 1\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        q_k_next = np.dot(q_k, stochastic_matrix)\n",
    "\n",
    "        if np.linalg.norm(q_k_next - q_k) < threshold:\n",
    "            break\n",
    "\n",
    "        q_k = q_k_next\n",
    "\n",
    "    return q_k_next\n",
    "\n",
    "\n",
    "def visualize_distributions_heatmap(\n",
    "    *distributions, node_labels=None, distribution_labels=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize multiple stationary distributions as a heatmap.\n",
    "\n",
    "    :param distributions: Stationary distributions obtained from random walks.\n",
    "    :param node_labels: Labels for the nodes (optional).\n",
    "    :param distribution_labels: Labels for the distributions (optional).\n",
    "    \"\"\"\n",
    "    data = np.vstack(distributions)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = sns.heatmap(\n",
    "        data,\n",
    "        annot=False,\n",
    "        cmap=\"viridis\",\n",
    "        yticklabels=distribution_labels\n",
    "        if distribution_labels\n",
    "        else [f\"Dist {i + 1}\" for i in range(len(distributions))],\n",
    "        xticklabels=node_labels if node_labels else [],\n",
    "    )\n",
    "    ax.set_title(\"Comparison of Stationary Distributions from Random Walks\")\n",
    "    ax.set_xlabel(\"Nodes\")\n",
    "    ax.set_ylabel(\"Distributions\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_distributions = []\n",
    "for i in range(3):\n",
    "    start_node = np.random.randint(stochastic_matrix.shape[0])\n",
    "    print(f\"Starting node: {start_node}\")\n",
    "    stationary_distribution = random_walk_without_restart_to_stationary_distribution(\n",
    "        stochastic_matrix, start_node\n",
    "    )\n",
    "    stationary_distributions.append(stationary_distribution)\n",
    "    print(f\"Finished computing stationary distribution for starting node {start_node}\")\n",
    "    print(f\"Sum of stationary distribution: {stationary_distribution.sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_distributions_heatmap(*stationary_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the heatmap above, the Random Walks Without Restart do not preserve the local information of the starting node, effectively annihilating the usefulness of RW without restart for node embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk With Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_with_restart_to_stationary_distribution(\n",
    "    stochastic_matrix,\n",
    "    start_node,\n",
    "    query_node,\n",
    "    continue_prob,\n",
    "    threshold=1e-10,\n",
    "    max_iterations=1000,\n",
    "    ignore_threshold=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a Random Walk with Restart on a graph represented by a stochastic matrix, starting from a given node,\n",
    "    until the stationary distribution is reached or the maximum number of iterations is exceeded.\n",
    "\n",
    "    :param stochastic_matrix: The stochastic matrix representing the graph.\n",
    "    :param start_node: The index of the starting node.\n",
    "    :param restart_prob: The probability of restarting to the initial node.\n",
    "    :param threshold: The threshold for the difference between successive distribution vectors.\n",
    "    :param max_iterations: The maximum number of iterations to perform.\n",
    "    :param ignore_threshold: Whether to ignore the threshold and always perform the maximum number of iterations.\n",
    "    :return: The stationary distribution vector of the random walk.\n",
    "    \"\"\"\n",
    "    num_nodes = stochastic_matrix.shape[0]\n",
    "\n",
    "    # Initialize the distribution vector q_k and the restart vector v\n",
    "    q_k = np.zeros(num_nodes)\n",
    "    q_k[start_node] = 1\n",
    "    v = np.zeros(num_nodes)\n",
    "    v[query_node] = 1\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i >= max_iterations:\n",
    "            print(\"Maximum number of iterations exceededed.\")\n",
    "            break\n",
    "\n",
    "        q_k_next = (\n",
    "            continue_prob * np.dot(q_k, stochastic_matrix) + (1 - continue_prob) * v\n",
    "        )\n",
    "\n",
    "        if np.linalg.norm(q_k_next - q_k) < threshold and not ignore_threshold:\n",
    "            break\n",
    "\n",
    "        q_k = q_k_next\n",
    "        i += 1\n",
    "\n",
    "    print(f\"Number of iterations: {i}\")\n",
    "\n",
    "    return q_k_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distributions_heatmap(\n",
    "    *distributions, node_labels=None, distribution_labels=None, cap_value=None, title=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize multiple stationary distributions as a heatmap, with an option to cap the highest value.\n",
    "\n",
    "    :param distributions: Stationary distributions obtained from random walks.\n",
    "    :param node_labels: Labels for the nodes (optional).\n",
    "    :param distribution_labels: Labels for the distributions (optional).\n",
    "    :param cap_value: The value to cap the highest value in the distributions to (optional).\n",
    "    \"\"\"\n",
    "    data = np.vstack(distributions)\n",
    "\n",
    "    # If cap_value is specified, cap the highest value in the distributions\n",
    "    if cap_value is not None:\n",
    "        data = np.where(data > cap_value, cap_value, data)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = sns.heatmap(\n",
    "        data,\n",
    "        annot=False,\n",
    "        cmap=\"viridis\",\n",
    "        yticklabels=distribution_labels\n",
    "        if distribution_labels\n",
    "        else [f\"Dist {i + 1}\" for i in range(data.shape[0])],\n",
    "        xticklabels=node_labels if node_labels else [],\n",
    "    )\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel(\"Nodes\")\n",
    "    ax.set_ylabel(\"Distributions\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_top_n_node_ranking_scatter_plots(\n",
    "    distributions, top_n=10, remove_first_n=None, labels=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize the top N node rankings in each distribution using scatter plots.\n",
    "\n",
    "    :param distributions: A list of distributions (numpy arrays).\n",
    "    :param top_n: The number of top nodes to consider.\n",
    "    :param remove_first_n: The number of top nodes to remove from visualization.\n",
    "    :param labels: Labels for each distribution.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for i, distribution in enumerate(distributions):\n",
    "        # Sort the distribution and pick the top N nodes\n",
    "        sorted_indices = np.argsort(distribution)\n",
    "\n",
    "        # Remove the first n nodes if specified\n",
    "        if remove_first_n is not None:\n",
    "            sorted_indices = sorted_indices[:-remove_first_n]\n",
    "\n",
    "        top_nodes_indices = sorted_indices[-top_n:]\n",
    "        top_nodes_values = distribution[top_nodes_indices]\n",
    "\n",
    "        # Plotting the top N nodes\n",
    "        plt.scatter(\n",
    "            top_nodes_indices,\n",
    "            top_nodes_values,\n",
    "            label=f\"Distribution {i+1}\" if labels is None else labels[i],\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Top {top_n} Node Rankings in Each Distribution\")\n",
    "    plt.xlabel(\"Node Index\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cdf(stationary_distributions, continue_probs, title=None, x_lim=None):\n",
    "    \"\"\"\n",
    "    Plot the Cumulative Distribution Function (CDF) for the stationary distributions of a Random Walk with Restart.\n",
    "\n",
    "    :param stationary_distributions: List of stationary distribution vectors for different continue probabilities.\n",
    "    :param continue_probs: List of continue probabilities corresponding to the stationary distributions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    for distribution, prob in zip(stationary_distributions, continue_probs):\n",
    "        sorted_distribution = np.sort(distribution)\n",
    "        cdf = np.cumsum(sorted_distribution)\n",
    "        \n",
    "        plt.plot(cdf, label=f'p = {prob}', linewidth=2)\n",
    "    \n",
    "    \n",
    "    plt.grid(True, which='both', axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title('Cumulative Distribution Function of Stationary Distributions')\n",
    "    if x_lim is not None:\n",
    "        plt.xlim(x_lim)\n",
    "    plt.xlabel('Node Index')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_1, node_2 = data.loc[0, \"node1\"], data.loc[0, \"node2\"]\n",
    "print(f\"Node 1: {node_1}\")\n",
    "print(f\"Node 2: {node_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarting nodes :\n",
    "* Node 1: ENSG00000284589\n",
    "* Node 2: ENSG00000276821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index of node 1 and node 2 in all nodes\n",
    "node_1_idx = np.where(all_nodes_cleaned == node_1)[0][0]\n",
    "node_2_idx = np.where(all_nodes_cleaned == node_2)[0][0]\n",
    "print(f\"Node 1 index: {node_1_idx}\")\n",
    "print(f\"Node 2 index: {node_2_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying restarting probability, fixed restarting node, fixed random starting node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first experiment we will fix N1 as the restart node, fix a random q0 as the starting node, and vary the probability of restarting from N1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_probs = [0.2, 0.5, 0.8]\n",
    "stationary_distributions = []\n",
    "start_node = 1022\n",
    "for prob in continue_probs:\n",
    "    print(f\"Starting node: {start_node}\")\n",
    "    print(f\"Restarting node: {node_2_idx}\")\n",
    "    print(f\"Restart probability: {1 - prob:.2f}\")\n",
    "    stationary_distribution = random_walk_with_restart_to_stationary_distribution(\n",
    "        stochastic_matrix, start_node, node_2_idx, prob, ignore_threshold=False\n",
    "    )\n",
    "    stationary_distributions.append(stationary_distribution)\n",
    "    print(f\"Finished computing stationary distribution for starting node {start_node}\")\n",
    "    print(f\"Sum of stationary distribution: {stationary_distribution.sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_distributions = np.array(stationary_distributions)\n",
    "stationary_distributions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(stationary_distributions, continue_probs,  x_lim=(2000, 11000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find second max value in each distribution\n",
    "second_max_values = []\n",
    "for distribution in stationary_distributions:\n",
    "    sorted_distribution = np.sort(distribution)\n",
    "    second_max_values.append(sorted_distribution[-2])\n",
    "\n",
    "second_max_values = np.array(second_max_values) * 100\n",
    "second_max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_distributions_heatmap(\n",
    "    *stationary_distributions,\n",
    "    cap_value=0.001,\n",
    "    distribution_labels=[f\"1 - p = {1 - prob:.1f}\" for prob in continue_probs],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_top_n_node_ranking_scatter_plots(\n",
    "    stationary_distributions,\n",
    "    top_n=7,\n",
    "    remove_first_n=1,\n",
    "    labels=[f\"1 - p = {1 - prob:.1f}\" for prob in continue_probs],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing $p=0.2$, varying starting node, fixed restarting node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_prob = 0.2\n",
    "stationary_distributions = []\n",
    "start_nodes = [1022, 7077, 3204]\n",
    "for start_node in start_nodes:\n",
    "    print(f\"Starting node: {start_node}\")\n",
    "    print(f\"Restarting node: {node_2_idx}\")\n",
    "    print(f\"Restart probability: {1 - prob:.2f}\")\n",
    "    stationary_distribution = random_walk_with_restart_to_stationary_distribution(\n",
    "        stochastic_matrix, start_node, node_2_idx, prob, ignore_threshold=False\n",
    "    )\n",
    "    stationary_distributions.append(stationary_distribution)\n",
    "    print(f\"Finished computing stationary distribution for starting node {start_node}\")\n",
    "    print(f\"Sum of stationary distribution: {stationary_distribution.sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_distributions_heatmap(*stationary_distributions, cap_value=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_top_n_node_ranking_scatter_plots(\n",
    "    stationary_distributions,\n",
    "    top_n=15,\n",
    "    remove_first_n=1,\n",
    "    labels=[f\"1 - p = {1 - prob:.1f}\" for prob in continue_probs],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing probability $p=0.2$ and Starting Node, Varying Restarting Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_prob = 0.2\n",
    "stationary_distributions = []\n",
    "start_nodes = 1022\n",
    "for restart_node in [node_1_idx, node_2_idx]:\n",
    "    print(f\"Starting node: {start_node}\")\n",
    "    print(f\"Restarting node: {restart_node}\")\n",
    "    print(f\"Restart probability: {1 - prob:.2f}\")\n",
    "    stationary_distribution = random_walk_with_restart_to_stationary_distribution(\n",
    "        stochastic_matrix, start_node, restart_node, prob, ignore_threshold=False\n",
    "    )\n",
    "    stationary_distributions.append(stationary_distribution)\n",
    "    print(f\"Finished computing stationary distribution for starting node {start_node}\")\n",
    "    print(f\"Sum of stationary distribution: {stationary_distribution.sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "sorted_distribution_1 = np.sort(stationary_distributions[0])\n",
    "cdf_1 = np.cumsum(sorted_distribution_1)\n",
    "sorted_distribution_2 = np.sort(stationary_distributions[1])\n",
    "cdf_2 = np.cumsum(sorted_distribution_2)\n",
    "\n",
    "plt.plot(cdf_1, label=f'Restarting node $N_1$', linewidth=2)\n",
    "plt.plot(cdf_2, label=f'Restarting node $N_2$', linewidth=2)\n",
    "\n",
    "\n",
    "plt.xlim(3000)\n",
    "plt.xlabel('Node Index', fontsize=14)\n",
    "plt.ylabel('Cumulative Probability', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True, which='both', axis='y', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate and compare node2vec embeddings with two different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "import joblib\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_embeddings(G, p, q, search_type):\n",
    "    node2vec = Node2Vec(G, dimensions=128, walk_length=80, num_walks=10, workers=8, p=p, q=q)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    model.wv.save_word2vec_format(f\"./graph_embed_{search_type}.emd\")\n",
    "    model.save(f\"./node2vec_{search_type}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_weighted_edgelist(\"./cleaned_graph.edge\")\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we use two sets of configurations :\n",
    "* (low p, low q) : depth first type of exploration\n",
    "* (high p, high q) : breadth first type of exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breadth first search\n",
    "generate_and_save_embeddings(G, 1, 1, \"bfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth first search\n",
    "generate_and_save_embeddings(G, 0.1, 0.1, \"dfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_embedding(file_path, node_name, node_names):\n",
    "    \"\"\"\n",
    "    Reads an embedding file and retrieves the embedding for a specific node.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the embedding file.\n",
    "    node_name (str): The name of the node whose embedding is to be retrieved.\n",
    "    node_names (list): A list of node names, indexed by their ID in the embedding file.\n",
    "\n",
    "    Returns:\n",
    "    list: The embedding of the specified node, or None if the node is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the index (ID) of the node\n",
    "        if node_name not in node_names:\n",
    "            return None\n",
    "        node_id = node_names.index(node_name)\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Skip the header line\n",
    "            next(file)\n",
    "\n",
    "            # Iterate through the file to find the matching node ID\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                if int(parts[0]) == node_id:\n",
    "                    # Return the embedding (excluding the node ID)\n",
    "                    return np.array([float(x) for x in parts[1:]])\n",
    "        \n",
    "        # Node ID not found in the file\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes_cleaned = joblib.load(\"./cleaned_nodes_arr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1_embedding_bfs = get_node_embedding(\"graph_embed_breadth_first.emd\", \"ENSG00000284589\", list(all_nodes_cleaned))\n",
    "n2_embedding_bfs = get_node_embedding(\"graph_embed_breadth_first.emd\", \"ENSG00000276821\", list(all_nodes_cleaned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1_embedding_dfs = get_node_embedding(\"graph_embed_depth_first.emd\", \"ENSG00000284589\", list(all_nodes_cleaned))\n",
    "n2_embedding_dfs = get_node_embedding(\"graph_embed_depth_first.emd\", \"ENSG00000276821\", list(all_nodes_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n1_embedding_bfs is not None and n1_embedding_dfs is not None:\n",
    "    pearson_corr = np.corrcoef(n1_embedding_bfs, n1_embedding_dfs)[0, 1]\n",
    "    spearman_corr, _ = spearmanr(n1_embedding_bfs, n1_embedding_dfs)\n",
    "    print(\"Spearman Correlation Coefficient:\", spearman_corr)\n",
    "    print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
    "else:\n",
    "    print(\"One or more embeddings not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n2_embedding_bfs is not None and n2_embedding_dfs is not None:\n",
    "    pearson_corr = np.corrcoef(n2_embedding_bfs, n2_embedding_dfs)[0, 1]\n",
    "    spearman_corr, _ = spearmanr(n2_embedding_bfs, n2_embedding_dfs)\n",
    "    print(\"Spearman Correlation Coefficient:\", spearman_corr)\n",
    "    print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
    "else:\n",
    "    print(\"One or more embeddings not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n1_embedding_bfs is not None and n2_embedding_bfs is not None:\n",
    "    pearson_corr = np.corrcoef(n1_embedding_bfs, n2_embedding_bfs)[0, 1]\n",
    "    spearman_corr, _ = spearmanr(n1_embedding_bfs, n2_embedding_bfs)\n",
    "    print(\"Spearman Correlation Coefficient:\", spearman_corr)\n",
    "    print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
    "else:\n",
    "    print(\"One or more embeddings not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n1_embedding_dfs is not None and n2_embedding_dfs is not None:\n",
    "    pearson_corr = np.corrcoef(n1_embedding_dfs, n2_embedding_dfs)[0, 1]\n",
    "    spearman_corr, _ = spearmanr(n1_embedding_dfs, n2_embedding_dfs)\n",
    "    print(\"Spearman Correlation Coefficient:\", spearman_corr)\n",
    "    print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
    "else:\n",
    "    print(\"One or more embeddings not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n1_embedding_bfs is not None and n2_embedding_dfs is not None:\n",
    "    pearson_corr = np.corrcoef(n1_embedding_bfs, n2_embedding_dfs)[0, 1]\n",
    "    spearman_corr, _ = spearmanr(n1_embedding_bfs, n2_embedding_dfs)\n",
    "    print(\"Spearman Correlation Coefficient:\", spearman_corr)\n",
    "    print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
    "else:\n",
    "    print(\"One or more embeddings not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n1_embedding_dfs is not None and n2_embedding_bfs is not None:\n",
    "    pearson_corr = np.corrcoef(n1_embedding_dfs, n2_embedding_bfs)[0, 1]\n",
    "    spearman_corr, _ = spearmanr(n1_embedding_dfs, n2_embedding_bfs)\n",
    "    print(\"Spearman Correlation Coefficient:\", spearman_corr)\n",
    "    print(\"Pearson Correlation Coefficient:\", pearson_corr)\n",
    "else:\n",
    "    print(\"One or more embeddings not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman Correlation Coefficients :\n",
    "\n",
    "| Correlation | N1 bfs | N1 dfs | N2 bfs | N2 dfs |\n",
    "|-------------|--------|--------|--------|--------|\n",
    "| N1 bfs      | 1      |        |        |        |\n",
    "| N1 dfs      | 0.046  | 1      |        |        |\n",
    "| N2 bfs      | 0.106  | 0.161  | 1      |        |\n",
    "| N2 dfs      | 0.037  | 0.119  | 0.078  | 1      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson Correlation Coefficients :\n",
    "\n",
    "| Correlation | N1 bfs | N1 dfs | N2 bfs | N2 dfs |\n",
    "|-------------|--------|--------|--------|--------|\n",
    "| N1 bfs      | 1      |        |        |        |\n",
    "| N1 dfs      | 0.051  | 1      |        |        |\n",
    "| N2 bfs      | 0.080  | 0.138  | 1      |        |\n",
    "| N2 dfs      | 0.051  | 0.128  | 0.088  | 1      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation according to p and q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the correlations as a function of p and q, then plot the results in a 3D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(correlation_type=\"pearson\"):\n",
    "    p_values = [0.1, 0.3, 0.7, 1.0]\n",
    "    q_values = [0.1, 0.3, 0.7, 1.0]\n",
    "    correlations = {'pearson': [], 'spearman': []}\n",
    "    node_names = list(joblib.load(\"./cleaned_nodes_arr.pkl\"))\n",
    "\n",
    "    for p in p_values:\n",
    "        for q in q_values:\n",
    "            generate_and_save_embeddings(G, p, q, f\"p{p}_q{q}\")\n",
    "            emb1 = get_node_embedding(f\"./graph_embed_p{p}_q{q}.emd\", \"ENSG00000284589\", node_names)\n",
    "            emb2 = get_node_embedding(f\"./graph_embed_p{p}_q{q}.emd\", \"ENSG00000276821\", node_names)\n",
    "\n",
    "            if emb1 is not None and emb2 is not None:\n",
    "                if correlation_type in [\"pearson\", \"both\"]:\n",
    "                    pearson_corr = np.corrcoef(emb1, emb2)[0, 1]\n",
    "                    correlations['pearson'].append((p, q, pearson_corr))\n",
    "                if correlation_type in [\"spearman\", \"both\"]:\n",
    "                    spearman_corr, _ = spearmanr(emb1, emb2)\n",
    "                    correlations['spearman'].append((p, q, spearman_corr))\n",
    "\n",
    "    return correlations if correlation_type == \"both\" else {correlation_type: correlations[correlation_type]}\n",
    "\n",
    "def plot_correlations(correlations_dict):\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    fig.patch.set_alpha(1.0)\n",
    "    \n",
    "    nrows, ncols = 1, 2\n",
    "    \n",
    "    scatter_plots = []\n",
    "\n",
    "    for i, (correlation_name, data) in enumerate(correlations_dict.items(), 1):\n",
    "        ax = fig.add_subplot(nrows, ncols, i, projection='3d')\n",
    "        ax.patch.set_facecolor('white')\n",
    "        ps = [p for p, _, _ in data]\n",
    "        qs = [q for _, q, _ in data]\n",
    "        corrs = [corr for _, _, corr in data]\n",
    "\n",
    "        scatter = ax.scatter(ps, qs, corrs, c=corrs, cmap='seismic')\n",
    "        scatter_plots.append(scatter)\n",
    "                \n",
    "        ax.set_xlabel('P value')\n",
    "        ax.set_ylabel('Q value')\n",
    "        ax.set_zlabel('Correlation')\n",
    "        ax.set_title(f'{correlation_name.capitalize()} Correlation')\n",
    "\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(scatter_plots[-1], cax=cbar_ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = compute_correlations(correlation_type=\"both\")\n",
    "joblib.dump(correlations, \"./correlations.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = joblib.load(\"./correlations.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('bmh'):\n",
    "    plot_correlations(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities, louvain_communities, girvan_newman\n",
    "import networkx as nx\n",
    "import random\n",
    "import joblib\n",
    "from time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_weighted_edgelist(\"./cleaned_graph.edge\")\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_clusters = list(greedy_modularity_communities(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_clusters = list(louvain_communities(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "girvan_clusters = list(girvan_newman(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = {\n",
    "    'greedy': greedy_clusters,\n",
    "    'louvain': louvain_clusters,\n",
    "    'girvan': girvan_clusters\n",
    "}\n",
    "\n",
    "joblib.dump(clusters_dict, \"./clusters.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_clusters = joblib.load(\"results/community_clusters/communities_greedy_modularity_communities.pkl\")\n",
    "louvain_clusters = joblib.load(\"results/community_clusters/communities_louvain.pkl\")\n",
    "len(greedy_clusters), len(louvain_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuttoff clusters with size <100 \n",
    "greedy_clusters = [cluster for cluster in greedy_clusters if len(cluster) >= 100]\n",
    "louvain_clusters = [cluster for cluster in louvain_clusters if len(cluster) >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_cluster_sizes = [len(cluster) for cluster in greedy_clusters]\n",
    "greedy_cluster_sizes.sort(reverse=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(list(range(len(greedy_cluster_sizes))), greedy_cluster_sizes)\n",
    "# plt.title(\"Histogram of Cluster Sizes for Greedy Modularity\")\n",
    "plt.xlabel(\"Cluster Index\")\n",
    "plt.ylabel(\"Cluster Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_cluster_sizes = [len(cluster) for cluster in louvain_clusters]\n",
    "louvain_cluster_sizes.sort(reverse=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(list(range(len(louvain_cluster_sizes))), louvain_cluster_sizes)\n",
    "# plt.title(\"Histogram of Cluster Sizes for Greedy Modularity\")\n",
    "plt.xlabel(\"Cluster Index\")\n",
    "plt.ylabel(\"Cluster Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Jaccard similarity score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return intersection / union\n",
    "\n",
    "# Initialize matrix to store Jaccard similarity values\n",
    "similarity_matrix = np.zeros((len(louvain_clusters), len(greedy_clusters)))\n",
    "\n",
    "# Compute Jaccard similarity between each pair of clusters from different algorithms\n",
    "for i, cluster1 in enumerate(louvain_clusters):\n",
    "    for j, cluster2 in enumerate(greedy_clusters):\n",
    "        sim = jaccard_similarity(cluster1, cluster2)\n",
    "        similarity_matrix[i, j] = sim\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "cax = ax.imshow(similarity_matrix, cmap='viridis', interpolation='nearest', aspect='auto')\n",
    "\n",
    "cbar = plt.colorbar(cax, pad=0.04, ticks=[np.nanmin(similarity_matrix), np.nanmax(similarity_matrix)])\n",
    "\n",
    "ax.set_xlabel('Clusters from Greedy Algorithm')\n",
    "ax.set_ylabel('Clusters from Louvain Algorithm')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Rand score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import rand_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_to_labels(clusters, num_nodes):\n",
    "    labels = [-1] * num_nodes\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for node_id in cluster:\n",
    "            labels[int(node_id)] = cluster_id\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes\n",
    "num_nodes = 10825\n",
    "\n",
    "# Convert the frozensets to lists of labels\n",
    "labels_algo1 = clusters_to_labels(louvain_clusters, num_nodes)\n",
    "labels_algo2 = clusters_to_labels(greedy_clusters, num_nodes)\n",
    "\n",
    "rand_index = rand_score(labels_algo1, labels_algo2)\n",
    "adjusted_rand_index = adjusted_rand_score(labels_algo1, labels_algo2)\n",
    "\n",
    "print(f\"Rand Index: {rand_index:.3f}\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand_index:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent the clusters visually on the graph network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "colors = list(mcolors.TABLEAU_COLORS.keys())\n",
    "num_colors = len(colors)\n",
    "num_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_clusters = joblib.load(\"results/community_clusters/communities_greedy_modularity_communities.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of clusters: {len(greedy_clusters)}\")\n",
    "print(f\"Size of biggest cluster: {len(greedy_clusters[0])}\")\n",
    "print(f\"Size of smallest cluster: {len(greedy_clusters[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort clusters by size, descending, and take the ten largest\n",
    "important_clusters = sorted(greedy_clusters, key=len, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color map for nodes in the ten largest clusters\n",
    "node_color_map = {}\n",
    "for cluster_id, cluster_nodes in enumerate(important_clusters):\n",
    "    for node in cluster_nodes:\n",
    "        node_color_map[node] = colors[cluster_id]  # Assign color to each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a default color for all other nodes\n",
    "default_color = 'grey'\n",
    "node_colors = [node_color_map.get(node, default_color) for node in G.nodes()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "plt.figure(figsize=(20, 20))\n",
    "pos = nx.spring_layout(G)  # positions for all nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/community_clusters/greedy_modularity_communities.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_clusters = sorted(greedy_clusters, key=len, reverse=True)[:10]\n",
    "for i, cluster in enumerate(largest_clusters):\n",
    "    largest_clusters[i] = set(cluster)\n",
    "\n",
    "# Create a subgraph for visualization\n",
    "nodes_to_include = set.union(*largest_clusters)\n",
    "subgraph = G.subgraph(nodes_to_include)\n",
    "\n",
    "# Use spring layout with constraints based on clusters\n",
    "pos = nx.spring_layout(subgraph, iterations=50)\n",
    "\n",
    "# Draw nodes\n",
    "for i, cluster in enumerate(largest_clusters):\n",
    "    nx.draw_networkx_nodes(subgraph, pos, nodelist=cluster, node_color=colors[i])\n",
    "\n",
    "# Draw edges with low opacity\n",
    "nx.draw_networkx_edges(subgraph, pos, alpha=0.1)\n",
    "\n",
    "# Hide axis\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.savefig('results/community_clusters/greedy_modularity_communities_subgraph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Girvan-Newman computational Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Girvan-Newman algorithm is very computationally expensive, we couldn't run it on the full graph. Let's extract subgraphs of different sizes and compute the time for the algorithm to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of results from each subgraph\n",
    "girvan_clusters_results = joblib.load('results/community_clusters/girvan_subgraphs_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = np.sort(np.array([res['size'] for res in girvan_clusters_results]) / 100)\n",
    "times = np.sort((np.array([res['time'] for res in girvan_clusters_results]) / 3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def exp_func(x, a, b, c):\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "params, _ = curve_fit(exp_func, sizes, times)\n",
    "\n",
    "extrapolated_sizes = np.linspace(0, 10, 100)\n",
    "extrapolated_times = exp_func(extrapolated_sizes, *params)\n",
    "\n",
    "with plt.style.context('seaborn-v0_8-whitegrid'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(sizes * 100, times, color='blue', label='Data')\n",
    "    plt.plot(extrapolated_sizes * 100, extrapolated_times, color='red', label='Exponential Curve')\n",
    "    plt.xlabel('Size of Subgraph (number of nodes)', fontsize=14)\n",
    "    plt.ylabel('Time (hours)', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    # plt.title('Extrapolation of Time vs Size')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_histories = joblib.load(\"./loss_histories_15_epochs.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_histories.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_losses(losses_dict, model_name, save=False):\n",
    "    train_loss = losses_dict['train_loss']\n",
    "    test_loss = losses_dict['test_loss']\n",
    "    activation_function = model_name.split('_')[0]\n",
    "    num_layers = model_name.split('_')[1]\n",
    "    with plt.style.context('seaborn-v0_8-darkgrid'):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(train_loss, label='Training Loss')\n",
    "        plt.plot(test_loss, label='Validation Loss')\n",
    "        # plt.title(f'Activation Function: {activation_function}, Layers: {num_layers}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(f'./assets/{model_name}.png')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, losses_dict in loss_histories.items():\n",
    "    plot_train_test_losses(losses_dict, model_name, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
